---
title: "Time Series Report"
output: 
  html_document:
    toc: true
    theme: default
    highlight: null
    css: styles.css
params:
  test: "dataframe"
  query: "query"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE, warning=FALSE, results='hide', message=FALSE}
#Loading the packages
#ipak <- function(pkg){
#  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
#  if (length(new.pkg))
#    install.packages(new.pkg, dependencies = TRUE)
#  sapply(pkg, require, character.only = TRUE)
#}

#packages <- c('fBasics', 'fUnitRoots', 'forecast', 'nnet', 'zoo')
#ipak(packages)

library(fBasics)
library(fUnitRoots)
library(forecast)
library(nnet)
library(zoo)
library(RJSONIO)
library(jsonlite)
```

```{r, echo=FALSE}
# test if there is at least one argument: if not, return an error
if (length(args)==0) {
  stop("At least one argument must be supplied (input file).\n", call.=FALSE)
} 

#data <-read.csv(params$args, header = TRUE)
data <- params$test
```

```{r, echo = F, warning = F, message = F}
message('data')
#message(data[,2])
message(colnames(data))
```


```{r, echo = FALSE}
#first thing to check: is the data is constant ? if so, we cannot do much!
constant <- (length(unique(na.omit(data[,2]))) == 1)
#period set to 1, to avoid duplication of eval arguments in the chunks
if (constant){
  period <- 1
  periodCorrelation <- 1
  pValue <- 0
  typ <- 'null'
  acf <- 0
  nullPresence <- FALSE
  negativePresence <-FALSE
  orderAR <- 0
}
```


```{r, echo=FALSE, eval = (constant == FALSE)}
#cuidado: lo primero que tengo que mirar es la longitud del periodo para poder crear la serie 
acf <- as.vector(acf(data[,2], plot = FALSE, na.action = na.pass )$acf)
```

```{r, echo = F, warning = F, message = F}
message('here')
message(constant)
message(acf)
```


```{r, echo = FALSE}
findingLocalMax <- function (acf){
  localMax <- -1
  for (i in 2:(length(acf) - 1)){
    previous <- acf[i -1]
    current <- acf[i]
    following <- acf[i + 1]
    
    if((previous < current) & (current > following)){
      localMax <- i
      break
    }
  }  
  return (localMax)
}
```

```{r, echo = FALSE, eval = (constant == FALSE)}
periodCorrelation <- findingLocalMax(acf)

period <- findfrequency(data[,2])
```


```{r, echo=FALSE}
ts <- ts(data[,2], frequency = period)
name <- colnames(data)[2]
```


```{r, echo = F, warning = F, message = F}
message('periodCorr')
message(periodCorrelation)

message('period')
message(period)
```

<div class="fluid-row" id="headerTitle"><h1 class="title toc-ignore"><span>Time series for column:</span>&nbsp;<strong>`r name`</strong></h1></div>

Mira ! `r print(params$query)`
Mira ! `r params$query`

Mira ! `r print(params[2])`

### Introduction

A report describing the series as thoroughly as possible is generated thereafter. The aim of this report is to extract key information about the series so that the user can take good decisions about his data. The report analyses the series and starts by performing simple transformations and then does more advanced checks. The ultimate goal of this functionnality is to automatically define alert thresholds in order to answer anomaly detection problematics. In the annex of this report there is a use case where the parameters extracted in the report are used to perform time series smoothing and to automatically detect anomalies. This report will ultimately be used for defining alerts in a simple and intuitive way.

### 1) Descriptive Analysis and Missing Values 

This time series represents the evolution of the number of `r name`. 

```{r, echo = FALSE}
plot(ts, ylab = name)
```

It contains `r length(ts)` observations, going from `r as.POSIXct(data[1,1]/1000, origin="1970-01-01")` to `r as.POSIXct(data[length(data[,1]),1]/1000, origin="1970-01-01")`. 

```{r, echo = FALSE, eval = (constant == TRUE)}
const <- unique(na.omit(data[,2]))
my_lag <- 1
```

`r if(constant == TRUE){paste0('The values of the time series are constant, all of them are equal to ', unique(na.omit(data[,2])), '. A constant series cannot be analyzed much more and so the report ends up here.')}`

```{r, echo = FALSE, eval = (constant == FALSE)}
nas <- sum(is.na(ts))

if(nas == 0){
  chunki <- 'This time series does **not** contain any **missing values**, thus there is no need to make any interpolation.'
}

if (nas != 0){
  chunki <- paste('This time series contains ', nas, ' **missing values**. The solution is to linearly interpolate the NAs with the values that wrap the missing value. Missing values in the extremes of the series (first or last values) will be refilled with the closest point.')
  
  ts <- na.approx(ts, rule = 2)
}

min_ts <- round(min(ts), 3)
max_ts <- round(max(ts), 3)
mean_ts <- round(mean(ts), 3)
```

`r if (constant == FALSE){paste0('The time series contains values between ', min_ts, ' and ', max_ts, ' with a mean of ', round(mean_ts,3), '.', separator = '')}`

`r if (constant == FALSE){chunki}`

<!-- `r if (constant == FALSE){'We will normalize the series in order to have all values between 0 and 1. This will be useful when comparing it with other time series that have different units. We display the plot of the normalized series.'}` -->

<!-- ```{r, echo = FALSE, eval = (constant == FALSE)} -->
<!-- ts2 <- (ts - min_ts)/(max_ts - min_ts) -->
<!-- plot(ts2, ylab = name) -->
<!-- ``` -->


`r if (constant == FALSE){'### 2) Computing the period length'}`

```{r, echo=FALSE, eval=(constant == FALSE)}
if(period == 1){
  chunk2 <- 'The period length is estimated with the **spectral density** technique and the result suggests that there is not a clear period in the time series. Unfortunately, nothing more can be said when the series is non-periodic so the report ends up here.'
}

if (period !=1){
  chunk2 <- 'In order to tackle the problem of period length detection two different techniques are used: one with autocorrelation and one with the spectral density.

**Autocorrelation:**

The autocorrelation of the original series with a lagged version of itself is calculated in order to get a candidate for the period length. The period length is defined as the lag index of the first local maximum in the autocorrelation plot.

The calculated coefficients are below:
'
  
}
pValue = 0
typ = 'null'
nullPresence <- FALSE
negativePresence <- FALSE
my_lag <- 1
orderAR <- 0
```

`r if(constant == FALSE) {chunk2}`

```{r, echo=FALSE, eval=(period!=1)}
plot(acf(ts, plot = FALSE), main = 'Autocorrelation with lagged versions')
legend('topright', legend = 'Confidence Interval', col = 'blue', cex = 0.8, lty = 2)
```

```{r, echo=FALSE, eval= (period!=1 & periodCorrelation != -1 )}
chunk2bis <- paste('The first local maximum and hence the period length is equal to ', periodCorrelation, '.', sep = '')
```

```{r, echo=FALSE, eval=(period!=1 & constant == FALSE & periodCorrelation == -1)}
chunk2bis <- paste('It is not possible to detect a local maximum in the bar plot so this method is discarded.')

```

```{r, echo=FALSE, eval=(period!=1)}
  chunk2bisbis <- '
**Spectral Density:**

This method takes into account the spectral density and it is based on the fact that the period is the inverse of the frequency, it takes as period length the frequency that corresponds to the maximum value of spectral density.'
```

`r if(period !=1) {chunk2bis}`

`r if(period !=1) {chunk2bisbis}`


```{r, echo=FALSE, eval=(period!=1)}
plot(spec.ar(ts, plot = FALSE), main = paste('Spectral Density of', name), ylab = 'Spectral Density')
```

`r if (period != 1) {paste('This method returns a period of length ', period, '. Recent studies have found, empirically, that in practice, this method performs better than the previous one. The result obtained with this method is kept for the future.', sep = '')}`

`r if(period !=1) {'### 3) Checking stationarity'}`

```{r, echo=FALSE, eval=(period!=1), message=FALSE, warning=FALSE}

res <- adfTest(ts, lags = 0, type = "nc")
pValue <- res@test$p.value
my_lag <- ndiffs(ts)

testResult <- function (pValue){
  if (pValue < 0.05){
    return(paste0('The **Dickey-Fuller** test tests the null hypothesis H0: ‘The series contains a unit root’ (i.e. is
not stationary). The alternative hypothesis is stationarity of the series. At a 0.05% level of confidence, the p-value (', round(pValue,3), ') is small and thus there is enough evidence to reject the null hypothesis and conclude that the series is indeed **stationary**.'));
  }

  else{
    return(paste0('The **Dickey-Fuller** test tests the null hypothesis H0: ‘The series contains a unit root’ (i.e. is
not stationary). The alternative hypothesis is stationarity of the series. At a 0.05% level of confidence, the p-value (', round(pValue,3), ') is bigger enough to fail to reject the null hypothesis and conclude that the series is **not stationary**.'));
  }
}

differentiating <- function(pValue){
    if (pValue < 0.05){
      return ('The is no need to differentiate the series. Subsequently, if an ARIMA(p,d,q) model is fitted, the d parameter that stands for the degree of differenciating will be equal to 0.')
    }
  else{
    return ('A way to make a series stationary is to differenciate it a certain number of times. The lagging of the series starts at level 1 and the lagging parameter is increased until the series is stationary.')
  }
}
```

`r if(period !=1) {testResult(pValue)}`

`r if(period !=1) {differentiating(pValue)}`


```{r, echo = F, warning = F, message = F}
message('ahora aqui')
message(ndiffs(ts))
```


```{r, echo=FALSE, eval=(period!=1 & pValue >= 0.05)}
if(my_lag == 0){my_lag = my_lag + 1}
plot(diff(ts, lag = my_lag), main = paste('Differentiation at lag = ', my_lag), ylab = 'Differentiated Series')
```

`r if(period !=1 & pValue >= 0.05 & my_lag == 1) {paste0('After ', my_lag, ' differenciation, the series is stationary. Subsequently, if an ARIMA(p,d,q) model is fitted, the d parameter that stands for the degree of differenciating will be equal to ', my_lag, '.')}`

`r if(period !=1 & pValue >= 0.05 & my_lag != 1) {paste0('After ', my_lag, ' differenciations, the series is already stationary. Subsequently, if an ARIMA(p,d,q) model is fitted, the d parameter that stands for the degree of differenciating will be equal to ', my_lag, '.')}`

`r if(period !=1) {'### 4) Decomposing the series'}`

```{r, echo=FALSE, eval=(period != 1), warning=FALSE}
#warning: null points are not allowed in the series as the estimation of the trend when using multiplicative type divides by 0. At first, we tried adding 0.00001 to the null values of the series. This idea distorted the result for the confidence intervals and had to be discarded. Finally, we decide to force the additive model on those cases.

nullPresence <- FALSE
negativePresence <- FALSE

if (0 %in% ts){
  acfAdd <- 0
  acfMult <- 0
  nullPresence <- TRUE
}

if (sum(ts < 0) != 0){
  acfAdd <- 0
  acfMult <- 0
  negativePresence <- TRUE
}
```


`r if(period !=1 & nullPresence == FALSE & (negativePresence == FALSE)) {'Time series decomposition is a powerful statistical method that decomposes a signal into several components (a trend, a periodic and a random component). This decomposition allows the user to isolate the seasonal component and make two decompositions: one with the additive assumption and one with the multiplicative one. This is done in order to check how much correlation between data points is still encoded within the residuals. The model with the lesser amount of autocorrelation in the random component is preferable as this will mean that the chosen model encapsulates as much information as possible.'}`


```{r, echo = F, warning = F, message = F}
message('nullPresence')
message(nullPresence)
message('negativePresence')
message(negativePresence)
```

`r if(period !=1 & nullPresence == TRUE & negativePresence == FALSE){'The time series contains values that are equal to 0. This means that a multiplicative model is not possible. The reason behind this is because in a multiplicative model the trend is calculated as division of two data points and thus a zero value is forbidden. Instead the **additive model** uses the substraction of data points and so it is used in this case.'}`

`r if(period !=1 & negativePresence == TRUE & nullPresence == FALSE){'The time series contains values that are negative. Thus, it makes more sense to use an **additive model**.'}`

`r if(period !=1 & negativePresence == TRUE & nullPresence == TRUE){'The time series contains values that are equal to 0 and also values that are below zero. This means that a multiplicative model is not possible. The reason behind this is because in a multiplicative model the trend is calculated as division of two data points and thus a zero value is forbidden. Instead the **additive model** uses the substraction of data points and so it is used in this case.'}`

`r if(period !=1 & nullPresence == FALSE & negativePresence == FALSE) {'The respective autocorrelations are plotted below:'}`


```{r, echo=FALSE, eval=(period!=1 & nullPresence == FALSE & negativePresence == FALSE)}
randomAdd <- na.approx(decompose(ts, type = "additive")$random, rule = 2)
randomMult <- na.approx(decompose(ts, type = "multiplicative")$random, rule = 2)

plot(acf(randomAdd, plot = FALSE), main ='Autocorrelation on the additive residuals')
legend('topright', legend = 'Confidence Interval', col = 'blue', cex = 0.8, lty = 2)

plot(acf(randomMult, plot = FALSE), main = 'Autocorrelation on the multiplicative residuals')
legend('topright', legend = 'Confidence Interval', col = 'blue', cex = 0.8, lty = 2)

```

```{r, echo=FALSE, eval=(period!=1 & nullPresence == FALSE & negativePresence == FALSE)}
acfAdd <- sum(acf(randomAdd, plot = FALSE)$acf^2)
acfMult <- sum(acf(randomMult, plot = FALSE)$acf^2)
```

```{r, echo=FALSE, eval=(period!=1)}
difference <- 0.01

addOrMult <- function(addCoeff, multCoeff){
  if ((abs(addCoeff-multCoeff) <=  difference) & (nullPresence == FALSE) & (negativePresence == FALSE)){
    res = paste0('In this case both autocorrelation coefficients are very **similar**. The additive one equals ', round(addCoeff,3), ' and the multiplicative component equals ', round(multCoeff,3), '. Choosing one is not an obvious task here so the rest of this report will display the solutions for both of the decompositions and the user can choose which one suits the problem best.' )
  }
  
  else{
    if (addCoeff < multCoeff){
    res = paste0('In this case the autocorrelation coefficient of the additive model (',round(addCoeff,3), ') is lesser than the multiplicative (',round(multCoeff,3),') so the conclusion is that fitting an **additive model** is more accurate.')
    }

    else{
    res = paste0('In this case the autocorrelation coefficient of the multiplicative model (',round(multCoeff,3),') is lesser than the additive (',round(addCoeff,3),') so the conclusion is that fitting a **multiplicative model** is more accurate.');
    }
  }
  return (res)
}

type <- function(addCoeff, multCoeff){
  if ((abs(addCoeff - multCoeff) <=  difference) & (nullPresence == FALSE) & (negativePresence == FALSE)){
    return('both')
  }
  else{
    if (addCoeff < multCoeff){
      return('additive')
    }

    else{
      return ('multiplicative')  
    }
  }
}

typ <- type(acfAdd, acfMult)
```

```{r, echo=FALSE, eval=(period != 1 & (nullPresence == TRUE | negativePresence == TRUE))}
typ <- 'additive'
```


`r if(period !=1 & nullPresence == FALSE & (negativePresence == FALSE)) {addOrMult(acfAdd, acfMult)}`

`r if(period !=1 & nullPresence == FALSE & (negativePresence == FALSE)) {'Now that the type of seasonality has been identified, the series can be decomposed'}`

```{r, echo = F, warning = F, message = F}
message('aqui')
message('type')
message(typ)
```

```{r, echo=FALSE, eval=((period != 1) & (typ != 'both') & (typ != 'null'))}
plot(decompose(ts, type = typ))
```

```{r, echo = F, warning = F, message = F}
message('aqui2')
message('type')
message(typ)
```


```{r, echo=FALSE, eval=((period != 1) & (typ == 'both'))}
plot(decompose(ts, type = 'additive'))
plot(decompose(ts, type = 'multiplicative'))
```

`r if(period !=1) {'### 5) Detecting the order of the Autoregressive process'}`

`r if(period !=1) {'It is also interesting to check the **partial autocorrelations** plot in order to have better insights on the relationship between the observations. Briefly, the partial autocorrelation at lag k is the correlation that results after removing the effect of any correlations due to the terms at shorter lags. Partial Autocorrelation plots are a commonly used for identifying the order of an autoregressive process AR(p). The order p is selected as the last lag that was statistically significant.'}`

```{r, echo=FALSE, eval=(period!=1)}
plot(pacf(ts, plot = FALSE), main = paste('Partial Autocorrelations of the', name, 'series'))
legend('topright', legend = 'Confidence Interval', col = 'blue', cex = 0.8, lty = 2, bty = "n")
```

```{r, echo=FALSE, eval=(period!=1)}
#extracting the last significant lag
upper <- 1.96 / sqrt(length(ts))
lower <- -1.96 / sqrt(length(ts))

findOrder <- function(pacf){
  i <- 1
  while((pacf$acf[i] > upper ) | (pacf$acf[i] < lower)){
    i = i + 1
  }
  return(i - 1)
}

orderAR <- findOrder(pacf(ts, plot = FALSE))
```

`r if(period !=1) {paste('The above plot shows that after the lag number ', orderAR, ' the values are no longer significative. That\'s why the underlying Auto Regressive process should be of order ', orderAR,'.', sep = '')}`

`r if(period !=1) {'### 6) Exponential Smoothing'}`

`r if(period !=1) {'In this section, the **Holt Winters** technique is applied to the series. Also called triple exponential smoothing, this method consists on smoothing the three components of our series.  It is also used to predict some points ahead into the future and to filter anomalies (if any).'}`

`r if(period !=1 & typ != 'both') {paste('The type of the smoothing is ', type(acfAdd, acfMult),' as it has already been detected in section 4). Predictions are made for two periods into the future, this means predicting ', period * 2 , ' points. After fitting the model, computing indicators of the goodness-of-fit is crucial. The AIC and the BIC are computed in this report. The indicators alone do not provide much information but they are useful for a future model comparison/selection.', sep = '')}`

`r if(period !=1 & typ == 'both') {paste('There wasn\'t a clear decomposition type in section 4) so exponential smoothing for both models is carried out. Predictions are made for two periods into the future, this means predicting ', period * 2 , ' points. After fitting the model, computing indicators of the goodness-of-fit is crucial. The AIC and the BIC are computed in this report. The indicators alone do not provide much information but they are useful for a future model comparison/selection.', sep = '')}`

`r if(period !=1) {'The **AIC** (Akaike Information Criterion) is an estimator of the relative quality of a model for a given set of data. Given a set of candidate models for the data, the preferred model is the one with the minimum AIC value. Thus, AIC rewards goodness of fit (as assessed by the likelihood function), but it also includes a penalty that is an increasing function of the number of estimated parameters. The penalty discourages overfitting, because increasing the number of parameters in the model almost always improves the goodness of the fit.'}`

`r if(period !=1) {'On the other hand, the **BIC** (Bayesian Information Criterion) indicator resemble the AIC (the lower the better) but with a penalty term larger than in AIC.'}`

```{r, echo = F, warning = F, message = F}
message('period')
message(period)

message('type')
message(typ)

message('neg')
message(negativePresence)
```


```{r, echo=FALSE, eval=((period != 1) & (typ != 'both') & (typ != 'null')), warning=FALSE}
hwinters <- HoltWinters(ts, seasonal = typ)
p <-predict(hwinters, n.ahead = period * 2, prediction.interval = TRUE)
plot(hwinters, p, main = paste(typ, 'H-W smoothing'))
legend('topleft', legend = c('True Series', 'Smoothing','Prediction Interval'), bty = "n", col = c('black', 'red', 'blue'), lty = c(1, 1, 1), inset = c(-0.025,-0.125), xpd = TRUE, horiz = TRUE)



if(typ == 'additive'){
  hwA <- hwinters
}
if(typ == 'multiplicative'){
  hwM <- hwinters
}
```


`r if((period !=1) & (typ != 'both')){paste('The **Nelder-Mead** optimization method is used in order to find the optimal values for the smoothing parameters. The results are: alpha = ', round(hwinters$alpha, 4), ', beta = ', round(hwinters$beta, 3), ' and gamma = ' , round(hwinters$gamma, 3), '.', sep = '')}`

```{r, echo=FALSE, eval=((period!=1) & (typ != 'both') & (typ != 'null'))}
t <- 'ZZA'
if(typ == 'multiplicative'){
  t <- 'ZZM'
}

if (period > 24){
  #the ets function does not allow us to fit a TS model when the period is too long that is why we use the arima technique
  fitArima <- auto.arima(ts)
  aic <- fitArima$aic
  aicc <- fitArima$aicc
  bic <- fitArima$bic
}

if (period <= 24){
  fitetsOne <- ets(ts, model = t)
  aic <- fitetsOne$aic
  aicc <- fitetsOne$aicc
  bic <- fitetsOne$bic
}

```

`r if((period !=1) & (typ != 'both')){paste('In this case, the AIC is equal to ', round(aic,3),' and the BIC equals ', round(bic,3), '. These indicators can be useful in a future model selection.', sep = '')}`

```{r, echo=FALSE, eval=((period != 1) & (typ == 'both')), warning=FALSE}
t <- 'ZZA'

hwA <- (HoltWinters(ts, seasonal = 'additive'))
pA <-predict(hwA, n.ahead = period * 2, prediction.interval = TRUE)
plot(hwA, pA, main ='Additive H-W smoothing')
legend('topleft', legend = c('True Series', 'Smoothing','Prediction Interval'), col = c('black', 'red', 'blue'), cex = 0.8, lty = 1, bty = "n")

if (period > 24){
  #the ets function does not allow us to fit a TS model when the period is too long that is why we use the arima technique
  fitArima <- auto.arima(ts)
  aica <- fitArima$aic
  aicc <- fitArima$aicc
  bic <- fitArima$bic
}

if (period <= 24){
  fitetsadd <- ets(ts, model = t)
  aica <- fitetsadd$aic
  aicc <- fitetsadd$aicc
  bic <- fitetsadd$bic
}
```

`r if((period !=1) & (typ == 'both')){paste('The **Nelder-Mead** optimization method is used in order to find the optimal values for the smoothing parameters. The results are: alpha = ', round(hwA$alpha, 4), ', beta = ', round(hwA$beta, 3), ' and gamma = ' , round(hwA$gamma, 3), '.', sep = '')}`

`r if((period !=1) & (typ == 'both')){paste('In the additive case, the AIC is equal to ', round(aica,3),' and the BIC equals ', round(bic,3), '. These indicators can be useful in a future model selection.', sep = '')}`

```{r, echo=FALSE, eval=((period != 1) & (typ == 'both')), warning=FALSE}
t <- 'ZZM'

hwM <- (HoltWinters(ts, seasonal = 'multiplicative'))
pM <-predict(hwM, n.ahead = period * 2, prediction.interval = TRUE)
plot(hwM, pM, main = 'Multiplicative H-W smoothing')
legend('topleft', legend = c('True Series', 'Smoothing','Prediction Interval'), col = c('black', 'red', 'blue'), cex = 0.8, lty = 1, bty = "n")

if (period > 24){
  #the ets function does not allow us to fit a TS model when the period is too long that is why we use the arima technique
  #fit <- auto.arima(ts)
  aicm <- fitArima$aic
  aicc <- fitArima$aicc
  bic <- fitArima$bic
}

if (period <= 24){
  fitetsmult <- ets(ts, model = t)
  aicm <- fitetsmult$aic
  aicc <- fitetsmult$aicc
  bic <- fitetsmult$bic
}

```

`r if((period !=1) & (typ == 'both')){paste('The **Nelder-Mead** optimization method is used in order to find the optimal values for the smoothing parameters. The results are: alpha = ',round(hwM$alpha, 4), ', beta = ', round(hwM$beta, 3), ' and gamma = ' , round(hwM$gamma, 3), '.', sep = '')}`

`r if((period !=1) & (typ == 'both')){paste('In the multiplicative case, the AIC is equal to ', round(aicm,3),' and the BIC equals ', round(bic,3), '. These indicators can be useful in a future model selection.', sep = '')}`


`r if(period !=1) {'### 7) Filtering Anomalies'}`

`r if(period !=1) {'For this matter, two different techniques are proposed in this report: percentiles and confidence intervals.'}`

```{r, echo=FALSE, eval = (period !=1) & (typ == 'both')}
lastType <- function(aica, aicm){
  if (aica < aicm){
    return('additive')
  }
  else{
    return('multiplicative')
  }
}

hwinters <- hwA
if (aica > aicm){
  hwinters <- hwM
}
```

`r if((period !=1) & (typ == 'both')) {paste('Choosing a model type has not been possible with automatic tools, so the model with the lowest AIC is selected, which is ', lastType(aica, aicm), '.', sep = '')}`


`r if(period !=1) {'The idea followed is to fit a Holt Winters smoothing as explained in the previous section in order to compute the residuals. The errors (i.e. the residuals) are calculated by taking the absolute value of the difference between the smoothing values and the true values. Their distribution is below.'}`

```{r, echo=FALSE, eval = ((period!=1) & (typ != 'both'))}
#we use the ets function here in order to have the same predictions length
#Warning: when the period is larger than 24 the ets function gives us an exception, we prefer to fit an arima model with the parameter D = 1 to take seasonal differenciating into account

if (period > 24){
  pred <- fitArima$fitted
}

if (period <= 24){
  pred <- fitetsOne$fitted
}

residuals<- abs(ts - pred)
hist(residuals, breaks = 50, main = 'Distribution of the residuals')
```

```{r, echo=FALSE, eval = ((period!=1) & (typ == 'both'))}
#we use the ets function here in order to have the same predictions length
t <- 'ZZA'
typ = 'additive'

if (aica > aicm){
  t <- 'ZZM'
  typ = 'multiplicative'
}

if (period > 24){
  pred <- fitArima$fitted
}

if (period <= 24){
  if (aica <= aicm){
      pred <- fitetsadd$fitted
  }
  else{
      pred <- fitetsmult$fitted
  }
}

residuals<- abs(ts - pred)
hist(residuals, breaks = 50, main = 'Distribution of the residuals')
```

`r if(period !=1) {'There is no normality assumption when fitting an exponential smoothing nor when producing confidence intervals, so a bell shaped histogram is not necessary.'}`

`r if(period !=1) {'**Using percentiles: **'}`

`r if(period !=1) {'This technique uses percentiles to determine which errors are too big to be considered normal. This is based on the heuristic that the values not explained by the model are anomalies. The 95 percentile is chosen for now but this can be set as a parameter in future versions.'}`

```{r, echo=FALSE, eval=(period!=1)}
threshold <- quantile(residuals, 0.95)
```

`r if(period !=1) {paste0('In brief, the points that produced residual errors that are beyond the 95 percentile will be taken as anomalies, this corresponds to the value ', round(threshold, 3), '. This means that an anomaly is found when the difference between the series and the smoothing is greater than the threshold value.', sep = '')}`

`r if(period !=1) {'The results are ploted below, the blue points are the detected anomalies.'}`

```{r, echo=FALSE, eval=(period!=1)}
plot(as.vector(ts), main = 'Percentiles', type = 'l', xlab = 'Time', ylab = '')
lines(1:length(ts), pred, col = 'red')
points(which(residuals > threshold), ts[(residuals > threshold)], pch = 19, col = 'blue')
legend('topleft', legend = c('Time Series', 'Prediction','Detected Anomaly'), bty = "n", col = c('black', 'red', 'blue'), lty = c(1, 1, 0), pch = c(NA, NA, 19), inset = c(-0.00,-0.125), xpd = TRUE, horiz = TRUE)

countAnomaliesPercentiles <- length(which(residuals > threshold))
```

`r if(period !=1) {paste0('This technique has identified ', countAnomaliesPercentiles, ' anomalous observations in the series.', sep = '')}`

`r if(period !=1) {'**Using confidence intervals: **'}`

`r if(period !=1) {'In this second part, another technique is used. A confidence interval (or a prediction interval) gives an interval within which the true value of the series is expected to lie with a specified probability. The confidence interval is built around the smoothing value and all points that lie outside the given interval are anomalies. Again, the blue points are the detected anomalies.'}`

```{r, echo=FALSE, eval=(period!=1)}
std_res <- sqrt(var(residuals))
upper<- as.vector(pred + std_res * 1.96)
lower<- as.vector(pred - std_res * 1.96)

plot(1:length(ts), lower, type = 'n', main = 'Confidence intervals', xlab = 'Time', ylab = 'Series', ylim = c(min(min(lower), min(ts)), max(max(upper), max(ts))))
lines(1:length(ts), upper, col = 'grey')
lines(1:length(ts), lower, col = 'grey')
polygon(c(1:length(ts), rev(1:length(ts))), c(upper, rev(lower)),
     col = "grey80", border = NA)
lines(1:length(ts), ts, col = 'black')
lines(1:length(ts), pred, col = 'red')
points(which((ts < lower) | (ts> upper)), ts[(ts < lower) | (ts > upper)], pch = 19, col = 'blue')

legend('topleft', legend = c('Time Series', 'Prediction','Detected Anomaly', 'Confidence Interval'), bty = "n", col = c('black', 'red', 'blue', NA), lty = c(1, 1, NA, NA), pch = c(NA, NA, 19, NA), density=c(0,0,0,NA), fill = c('black', 'red', 'blue', 'grey'), border = c(NA,NA,NA,"grey"), inset = c(0.658,-0.3), xpd = TRUE, horiz = FALSE)

countAnomaliesConfidence <- length(which((ts < lower) | (ts> upper)))
```

`r if(period !=1) {paste0('This technique has identified ', countAnomaliesConfidence, ' anomalous observations in the series.', sep = '')}`

```{r, echo=FALSE, eval = FALSE}
#We build the json object that contains all parameters necessary to the subsequent study.
list1 <- vector(mode="list")
list1[['period']] <- period
list1[['pvalue']] <- pValue
list1[['d']] <- my_lag
list1[['p']] <- orderAR
```

```{r, echo=FALSE, eval = FALSE}
#We build the json object that contains all parameters necessary to the subsequent study.
list1[['type']] <- typ
list1[['alpha']] <- round(hwinters$alpha, 4)
list1[['beta']] <- round(hwinters$beta, 4)
list1[['gamma']] <- round(hwinters$gamma, 4)
list1[['thresholdPercentile']] <- threshold
list1[['thresholdConfInt']] <- std_res
```

```{r, echo=FALSE, eval = FALSE}
list1[['type']] <- typ
list1[['alphaA']] <- round(hwA$alpha, 4)
list1[['betaA']] <- round(hwA$beta, 4)
list1[['gammaA']] <- round(hwA$gamma, 4)
list1[['alphaM']] <- round(hwM$alpha, 4)
list1[['betaM']] <- round(hwM$beta, 4)
list1[['gammaM']] <- round(hwM$gamma, 4)
list1[['chosenType']] <- lastType(aica, aicm)
list1[['thresholdPercentile']] <- threshold
list1[['thresholdConfInt']] <- std_res
```

```{r, echo=FALSE, eval = FALSE}
exportJson <- toJSON(list1, pretty = TRUE)
```

```{r, echo=FALSE, eval=FALSE}
write(exportJson, "test.json")
```

`r if(period !=1) {'### Annex: Use Cases'}`

`r if(period !=1) {'This report ends with a simple use case so that the user can get an idea of how to proceed with the knowledge he has acquired with the report. The period length and the three smoothing parameters are used to perform a Holt Winters smoothing automatically, afterwards, the thresholds computed in the report are used to detect anomalies and thus define alerts.'}` 

`r if(period !=1) {'The ts argument refers to the series we are analysing. It can be extrated from the present query by selecting the column specified in the title of the report.'}` 

`r if(period !=1) {'The code is available in two languages: R and Python.'}` 

`r if(period !=1) {'#### R Code'}`

```{r comment=NA, echo=FALSE, eval=(period!=1)}
message('period <- ', period, '\nalpha <- ', hwinters$alpha, '\nbeta <- ', hwinters$beta,
        '\ngamma <- ', hwinters$gamma, '\ntype <- ', t, '\nthresholdPercentile <- ', threshold,
        '\nthresholdConfInt <- ', std_res)
```

```{r, eval=FALSE}
smoothing <- ets(ts, alpha = alpha, beta = beta, gamma = gamma, model = t)
total_table <- cbind(ts, smoothing$fitted)
total_table <- cbind(total_table, thresholdPercentile)
total_table <- cbind(total_table, thresholdConfInt)
colnames(total_table) <- c('trueValue', 'predictions', 'thresholdPercentile', 'thresholdConfInt')
```

`r if(period !=1) {'#### Python Code'}`

```{r comment=NA, echo=FALSE, eval=(period!=1)}
message('period = ', period, '\nalpha = ', hwinters$alpha, '\nbeta = ', hwinters$beta,
        '\ngamma = ', hwinters$gamma, '\ntype = ', t, '\nthresholdPercentile = ', threshold,
        '\nthresholdConfInt = ', std_res)
```

```{r, eval=FALSE}
ets_model = ExponentialSmoothing(ts, trend='add', seasonal='add', 
seasonal_periods=period)
smoothing = ets_model.fit(smoothing_level=alpha, smoothing_slope=beta, smoothing_seasonal=gamma)

percentileThreshold = total_table['thresholdPercentile']
thresholdConfInf = total_table['thresholdConfInt']
total_table= pd.concat([ts, smoothing.fittedvalues], axis = 1)
total_table = total_table.rename(columns={'max_open_files': 'trueValue', 0: 'predictions'})
total_table['thresholdPercentile'] = thresholdPercentile
total_table['thresholdConfInt'] = thresholdConfInt
```


